# 7.rpcenv

> RPC 介绍：http://blog.csdn.net/mindfloating/article/details/39473807

## 7.1 上下文

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/SparkEnv.scala

private def create(
    conf: SparkConf,
    executorId: String,
    bindAddress: String,
    advertiseAddress: String,
    port: Int,
    isLocal: Boolean,
    numUsableCores: Int,
    ioEncryptionKey: Option[Array[Byte]],
    listenerBus: LiveListenerBus = null,
    mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv = {
    
    val rpcEnv = RpcEnv.create(systemName, bindAddress, advertiseAddress, port, conf,
        securityManager, clientMode = !isDriver)
        
    // Figure out which port RpcEnv actually bound to in case the original port is 0 or occupied.
    // In the non-driver case, the RPC env's address may be null since it may not be listening
    // for incoming connections.
    if (isDriver) {
        conf.set("spark.driver.port", rpcEnv.address.port.toString)
    } else if (rpcEnv.address != null) {
        conf.set("spark.executor.port", rpcEnv.address.port.toString)
        logInfo(s"Setting spark.executor.port to: ${rpcEnv.address.port.toString}")
    }
    
    def registerOrLookupEndpoint(
        name: String, endpointCreator: => RpcEndpoint):
        RpcEndpointRef = {
        
        if (isDriver) {
            logInfo("Registering " + name)
            rpcEnv.setupEndpoint(name, endpointCreator)
        } else {
            RpcUtils.makeDriverRef(name, conf, rpcEnv)
        }
    }
    
    // Have to assign trackerEndpoint after initialization as MapOutputTrackerEndpoint
    // requires the MapOutputTracker itself
    mapOutputTracker.trackerEndpoint = registerOrLookupEndpoint(MapOutputTracker.ENDPOINT_NAME,
        new MapOutputTrackerMasterEndpoint(
            rpcEnv, mapOutputTracker.asInstanceOf[MapOutputTrackerMaster], conf))
            
    val blockManagerMaster = new BlockManagerMaster(registerOrLookupEndpoint(
        BlockManagerMaster.DRIVER_ENDPOINT_NAME,
        new BlockManagerMasterEndpoint(rpcEnv, isLocal, conf, listenerBus)),
        conf, isDriver)
        
    // NB: blockManager is not valid until initialize() is called later.
    val blockManager = new BlockManager(executorId, rpcEnv, blockManagerMaster,
        serializerManager, conf, memoryManager, mapOutputTracker, shuffleManager,
        blockTransferService, securityManager, numUsableCores)
        
    val outputCommitCoordinatorRef = registerOrLookupEndpoint("OutputCommitCoordinator",
        new OutputCommitCoordinatorEndpoint(rpcEnv, outputCommitCoordinator))
        
    val envInstance = new SparkEnv(
        executorId,
        rpcEnv,
        serializer,
        closureSerializer,
        serializerManager,
        mapOutputTracker,
        shuffleManager,
        broadcastManager,
        blockManager,
        securityManager,
        metricsSystem,
        memoryManager,
        outputCommitCoordinator,
        conf)
```

## 7.2 RpcEnv.create

> 在 `SparkEnv` 中调用 `RpcEnv.create(systemName, bindAddress, advertiseAddress, port, conf, securityManager, clientMode = !isDriver)` 初始化 `RpcEnv` 类。

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala

private[spark] object RpcEnv {

    def create(
        name: String,
        bindAddress: String,
        advertiseAddress: String,
        port: Int,
        conf: SparkConf,
        securityManager: SecurityManager,
        clientMode: Boolean): RpcEnv = {
        val config = RpcEnvConfig(conf, name, bindAddress, advertiseAddress, port, securityManager,
            clientMode)
            
        // 最后会调用 NettyRpcEnvFactory 创建 RpcEnv ，入参为 RpcEnvConfig 类。
        // RpcEnvConfig 包含 SparkConf, name, bindAddress, advertiseAddress, port, securityManager, clientMode
        new NettyRpcEnvFactory().create(config)
    }
}

private[spark] case class RpcEnvConfig(
    conf: SparkConf,
    name: String,
    bindAddress: String,
    advertiseAddress: String,
    port: Int,
    securityManager: SecurityManager,
    clientMode: Boolean)
```

## 7.3 NettyRpcEnvFactory().create

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala

// 继承了 RpcEnvFactory 。
// RpcEnvFactory 是一个接口（trait），继承该类的所有类必须实现一个 create 的方法。
private[rpc] class NettyRpcEnvFactory extends RpcEnvFactory with Logging {

    def create(config: RpcEnvConfig): RpcEnv = {
        val sparkConf = config.conf
        // Use JavaSerializerInstance in multiple threads is safe. However, if we plan to support
        // KryoSerializer in future, we have to use ThreadLocal to store SerializerInstance
        val javaSerializerInstance =
        new JavaSerializer(sparkConf).newInstance().asInstanceOf[JavaSerializerInstance]
        val nettyEnv =
            new NettyRpcEnv(sparkConf, javaSerializerInstance, config.advertiseAddress,
                config.securityManager)
        if (!config.clientMode) {
            val startNettyRpcEnv: Int => (NettyRpcEnv, Int) = { actualPort =>
                nettyEnv.startServer(config.bindAddress, actualPort)
                (nettyEnv, nettyEnv.address.port)
            }
            try {
                Utils.startServiceOnPort(config.port, startNettyRpcEnv, sparkConf, config.name)._1
            } catch {
                case NonFatal(e) =>
                    nettyEnv.shutdown()
                    throw e
            }
        }
        nettyEnv
    }
}
```

## 7.4 NettyRpcEnv

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala

// 继承了 RpcEnv 类。
// RpcEnv 是一个抽象类，定义了一系列方法。
private[netty] class NettyRpcEnv(
    val conf: SparkConf,
    javaSerializerInstance: JavaSerializerInstance,
    host: String,
    securityManager: SecurityManager) extends RpcEnv(conf) with Logging {
    
    // 调用关系：
    // SparkTransportConf.fromSparkConf -> new TransportConf()
    // SparkTransportConf.fromSparkConf 第一步会设置使用线程
    //   - 若 spark.rpc.io.threads 配置大于0，则使用它和 MAX_DEFAULT_NETTY_THREADS 比更小的一个值。
    //   - 若 spark.rpc.io.threads 配置没有配置，则使用当前可用的线程 和 MAX_DEFAULT_NETTY_THREADS 比更小的一个值。
    //   - MAX_DEFAULT_NETTY_THREADS 这个值默认为8，也就是说最大使用8个线程。
    //   - 设置 spark.rpc.io.serverThreads 配置为 使用线程数
    //   - 设置 spark.rpc.io.clientThreads 配置为 使用线程数
    // 
    // 最后返回一个 TransportConf 类的实例。
    // TransportConf 会根据第二个入参（这里指 "rpc" ）获取一系列配置。
    //   - spark.rpc.io.mode
    //   - spark.rpc.io.preferDirectBufs
    //   - spark.rpc.io.connectionTimeout
    //   - spark.rpc.io.backLog
    //   - spark.rpc.io.numConnectionsPerPeer
    //   - spark.rpc.io.serverThreads
    //   - spark.rpc.io.clientThreads
    //   - spark.rpc.io.receiveBuffer
    //   - spark.rpc.io.sendBuffer
    //   - spark.rpc.sasl.timeout
    //   - spark.rpc.io.maxRetries
    //   - spark.rpc.io.retryWait
    //   - spark.rpc.io.lazyFD
    private[netty] val transportConf = SparkTransportConf.fromSparkConf(
        conf.clone.set("spark.rpc.io.numConnectionsPerPeer", "1"),
        "rpc",
        conf.getInt("spark.rpc.io.threads", 0))
        
    // 返回一个 Dispatcher 类的实例。
    // #?# 未看
    private val dispatcher: Dispatcher = new Dispatcher(this)

    // 返回 NettyStreamManager 类实例。
    // NettyStreamManager 继承 StreamManager(abstract class), RpcEnvFileServer(trait)，并初始化三个变量。
    //   - private val files = new ConcurrentHashMap[String, File]()
    //   - private val jars = new ConcurrentHashMap[String, File]()
    //   - private val dirs = new ConcurrentHashMap[String, File]()
    private val streamManager = new NettyStreamManager(this)
    
    private val transportContext = new TransportContext(transportConf,
        new NettyRpcHandler(dispatcher, this, streamManager))
    
    private def createClientBootstraps(): java.util.List[TransportClientBootstrap] = {
        if (securityManager.isAuthenticationEnabled()) {
            java.util.Arrays.asList(new AuthClientBootstrap(transportConf,
            securityManager.getSaslUser(), securityManager))
        } else {
            java.util.Collections.emptyList[TransportClientBootstrap]
        }
    }
    
    private val clientFactory = transportContext.createClientFactory(createClientBootstraps())
    
    /**
    * A separate client factory for file downloads. This avoids using the same RPC handler as
    * the main RPC context, so that events caused by these clients are kept isolated from the
    * main RPC traffic.
    *
    * It also allows for different configuration of certain properties, such as the number of
    * connections per peer.
    */
    @volatile private var fileDownloadFactory: TransportClientFactory = _
    
    val timeoutScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor("netty-rpc-env-timeout")
    
    // Because TransportClientFactory.createClient is blocking, we need to run it in this thread pool
    // to implement non-blocking send/ask.
    // TODO: a non-blocking TransportClientFactory.createClient in future
    private[netty] val clientConnectionExecutor = ThreadUtils.newDaemonCachedThreadPool(
        "netty-rpc-connection",
        conf.getInt("spark.rpc.connect.threads", 64))
        
    @volatile private var server: TransportServer = _
    
    private val stopped = new AtomicBoolean(false)
        
    /**
    * A map for [[RpcAddress]] and [[Outbox]]. When we are connecting to a remote [[RpcAddress]],
    * we just put messages to its [[Outbox]] to implement a non-blocking `send` method.
    */
    private val outboxes = new ConcurrentHashMap[RpcAddress, Outbox]()
}    
```



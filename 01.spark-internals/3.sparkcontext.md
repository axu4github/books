# 3.sparkcontext

在 `sparksession` 章节初始化 `SparkSession` 类时第一步就是调用 `SparkContext.getOrCreate()` 方法初始化 `SparkContext`。

```scala
// ${SPARK_HOME}/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala

@InterfaceStability.Stable
object SparkSession {
	@InterfaceStability.Stable
	class Builder extends Logging {
	  
		private[this] var userSuppliedContext: Option[SparkContext] = None
		    
		def getOrCreate(): SparkSession = synchronized {
			SparkSession.synchronized {
				val sparkContext = userSuppliedContext.getOrElse {
					val sparkConf = new SparkConf()
					val sc = SparkContext.getOrCreate(sparkConf)
					sc
				} 
			}
		}
	}
}
```

`SparkContext.getOrCreate()` 是 `SparkContext 伴生对象的方法` 以下是该方法部分代码。

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/SparkContext.scala

// SparkContext 的伴生对象
object SparkContext extends Logging {
	
	private val SPARK_CONTEXT_CONSTRUCTOR_LOCK = new Object()
	
	private val activeContext: AtomicReference[SparkContext] =
		new AtomicReference[SparkContext](null)

	def getOrCreate(config: SparkConf): SparkContext = {
		// 确保只有一个启动（active）的 SparkContext 实例。
		SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
			if (activeContext.get() == null) {
				// setActiveContext 会调用 assertNoOtherContextIsRunning 函数进一步确保在当前JVM中没有已经启动的 SparkContext 实例。
				setActiveContext(new SparkContext(config), allowMultipleContexts = false)
			} else {
				if (config.getAll.nonEmpty) {
					logWarning("Using an existing SparkContext; some configuration may not take effect.")
				}
			}
			activeContext.get()
		}
	}
	  
	private[spark] def setActiveContext(
		sc: SparkContext,
		allowMultipleContexts: Boolean): Unit = {
		SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
			assertNoOtherContextIsRunning(sc, allowMultipleContexts)
			contextBeingConstructed = None
			activeContext.set(sc)
		}
	}
  
	private def assertNoOtherContextIsRunning(
		sc: SparkContext,
		allowMultipleContexts: Boolean): Unit = {
		SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
			Option(activeContext.get()).filter(_ ne sc).foreach { ctx =>
				[...]
				if (allowMultipleContexts) {
					logWarning("Multiple running SparkContexts detected in the same JVM!", exception)
				} else {
					throw exception
				}
			}
		}
	}
}
```

真正初始化 `SparkContext` 类的语句是 `setActiveContext(new SparkContext(config), allowMultipleContexts = false)`，下面进入 `SparkContext` 类中看看都干了什么。

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/SparkContext.scala

class SparkContext(config: SparkConf) extends Logging {

  private[spark] val listenerBus = new LiveListenerBus(this)
    
  _conf = config.clone()
    
  _jars = Utils.getUserJars(_conf)
    
  _files =
    _conf.getOption("spark.files").map(_.split(",")).map(_.filter(_.nonEmpty)).toSeq.flatten    
  
  _eventLogDir =
    if (isEventLogEnabled) {
      val unresolvedDir = conf.get("spark.eventLog.dir", EventLoggingListener.DEFAULT_LOG_DIR)
        .stripSuffix("/")
      Some(Utils.resolveURI(unresolvedDir))
    } else {
      None
    }

  _eventLogCodec = {
    val compress = _conf.getBoolean("spark.eventLog.compress", false)
    if (compress && isEventLogEnabled) {
      Some(CompressionCodec.getCodecName(_conf)).map(CompressionCodec.getShortName)
    } else {
      None
    }
  }
    
  _jobProgressListener = new JobProgressListener(_conf)
  listenerBus.addListener(jobProgressListener)
    
  _env = createSparkEnv(_conf, isLocal, listenerBus)
  SparkEnv.set(_env)
  
  _statusTracker = new SparkStatusTracker(this)
  
  _ui =
    if (conf.getBoolean("spark.ui.enabled", true)) {
      Some(SparkUI.createLiveUI(this, _conf, listenerBus, _jobProgressListener,
        _env.securityManager, appName, startTime = startTime))
    } else {
      // For tests, do not enable the UI
      None
    }
  _ui.foreach(_.bind())
    
  _hadoopConfiguration = SparkHadoopUtil.get.newConfiguration(_conf)
    
  _executorMemory = _conf.getOption("spark.executor.memory")
    .orElse(Option(System.getenv("SPARK_EXECUTOR_MEMORY")))
    .orElse(Option(System.getenv("SPARK_MEM"))
    .map(warnSparkMem))
    .map(Utils.memoryStringToMb)
    .getOrElse(1024)
      
  _heartbeatReceiver = env.rpcEnv.setupEndpoint(
      HeartbeatReceiver.ENDPOINT_NAME, new HeartbeatReceiver(this))
  val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
    _schedulerBackend = sched
    _taskScheduler = ts
    _dagScheduler = new DAGScheduler(this)
    _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)
  _taskScheduler.start()
  
  _env.blockManager.initialize(_applicationId)
  
  _env.metricsSystem.start()
  
  _cleaner =
    if (_conf.getBoolean("spark.cleaner.referenceTracking", true)) {
      Some(new ContextCleaner(this))
    } else {
      None
    }
  _cleaner.foreach(_.start())
    
  setupAndStartListenerBus()
  postEnvironmentUpdate()
  postApplicationStart()
    
  _taskScheduler.postStartHook()
  _env.metricsSystem.registerSource(_dagScheduler.metricsSource)
  _env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager))
  _executorAllocationManager.foreach { e =>
    _env.metricsSystem.registerSource(e.executorAllocationManagerSource)
  }

  _shutdownHookRef = ShutdownHookManager.addShutdownHook(
    ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY) { () =>
    logInfo("Invoking stop() from shutdown hook")
    stop()
  }
}
```



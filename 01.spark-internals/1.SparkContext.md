# 1. SparkContext

## 1.1 spark-shell

```bash
> echo $JAVA_HOME
/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home

> cd /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home
> ll
total 52144
-rw-rw-r--   1 root  wheel      3244  9 14 17:50 COPYRIGHT
-rw-rw-r--   1 root  wheel        40  9 14 17:50 LICENSE
-rw-rw-r--   1 root  wheel       159  9 14 17:50 README.html
-rwxrwxr-x   1 root  wheel     63933  9 14 13:48 THIRDPARTYLICENSEREADME-JAVAFX.txt
-rw-rw-r--   1 root  wheel    145180  9 14 17:50 THIRDPARTYLICENSEREADME.txt
drwxrwxr-x  46 root  wheel      1564  9 14 17:55 bin
drwxrwxr-x   9 root  wheel       306  9 14 17:50 db
drwxrwxr-x   9 root  wheel       306  9 14 17:50 include
-rwxrwxr-x   1 root  wheel   5202879  9 14 13:48 javafx-src.zip
drwxrwxr-x  10 root  wheel       340  9 14 17:52 jre
drwxrwxr-x  14 root  wheel       476  9 14 17:52 lib
drwxrwxr-x   5 root  wheel       170  9 14 17:50 man
-rw-rw-r--   1 root  wheel       427  9 14 17:50 release
-rw-rw-r--   1 root  wheel  21258665  9 14 17:50 src.zip

> bin/jvisualvm
```

![](assets/markdown-img-paste-20180221215222654.png)

> 我的电脑上已经启动了 `Spark Standalone` 模式（`org.apache.spark.deploy.master.Master && org.apache.spark.deploy.worker.Worker`）

```bash
> echo $SPARK_HOME
/Users/axu/opt/apache-spark/latest

> cd /Users/axu/opt/apache-spark/latest
> bin/spark-shell --master spark://localhost:7077
[...]
scala>
```

![](assets/markdown-img-paste-20180221220936932.png)

![](assets/markdown-img-paste-20180221221018808.png)

![](assets/markdown-img-paste-20180221221106321.png)

```java
"main" #1 prio=5 os_prio=31 tid=0x00007fd89f000800 nid=0x2003 runnable [0x000070000f458000]
   java.lang.Thread.State: RUNNABLE
      at java.io.FileInputStream.read0(Native Method)
      at java.io.FileInputStream.read(FileInputStream.java:207)
      at jline.internal.NonBlockingInputStream.read(NonBlockingInputStream.java:169)
      - locked <0x000000078001e740> (a jline.internal.NonBlockingInputStream)
      at jline.internal.NonBlockingInputStream.read(NonBlockingInputStream.java:137)
      at jline.internal.NonBlockingInputStream.read(NonBlockingInputStream.java:246)
      at jline.internal.InputStreamReader.read(InputStreamReader.java:261)
      - locked <0x000000078001e740> (a jline.internal.NonBlockingInputStream)
      at jline.internal.InputStreamReader.read(InputStreamReader.java:198)
      - locked <0x000000078001e740> (a jline.internal.NonBlockingInputStream)
      at jline.console.ConsoleReader.readCharacter(ConsoleReader.java:2145)
      at jline.console.ConsoleReader.readLine(ConsoleReader.java:2349)
      at jline.console.ConsoleReader.readLine(ConsoleReader.java:2269)
      at scala.tools.nsc.interpreter.jline.InteractiveReader.readOneLine(JLineReader.scala:57)
      at scala.tools.nsc.interpreter.InteractiveReader$$anonfun$readLine$2.apply(InteractiveReader.scala:37)
      at scala.tools.nsc.interpreter.InteractiveReader$$anonfun$readLine$2.apply(InteractiveReader.scala:37)
      at scala.tools.nsc.interpreter.InteractiveReader$.restartSysCalls(InteractiveReader.scala:44)
      at scala.tools.nsc.interpreter.InteractiveReader$class.readLine(InteractiveReader.scala:37)
      at scala.tools.nsc.interpreter.jline.InteractiveReader.readLine(JLineReader.scala:28)
      at scala.tools.nsc.interpreter.ILoop.readOneLine(ILoop.scala:404)
      at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:413)
      at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:923)
      at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)
      at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)
      at scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97)
      at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:909)
      at org.apache.spark.repl.Main$.doMain(Main.scala:74)
      at org.apache.spark.repl.Main$.main(Main.scala:54)
      at org.apache.spark.repl.Main.main(Main.scala)
      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
      at java.lang.reflect.Method.invoke(Method.java:498)
      at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775)
      at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
      at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
      at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
      at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
```

> 去掉非Spark源码部分，spark-shell 启动这件事和两个文件有关：
> - org.apache.spark.deploy.SparkSubmit (SparkSubmit.scala)
> - org.apache.spark.repl.Main (Main.scala)

```java
"main" #1 prio=5 os_prio=31 tid=0x00007fd89f000800 nid=0x2003 runnable [0x000070000f458000]
   java.lang.Thread.State: RUNNABLE
      [...]
      at org.apache.spark.repl.Main$.doMain(Main.scala:74)
      at org.apache.spark.repl.Main$.main(Main.scala:54)
      at org.apache.spark.repl.Main.main(Main.scala)
      [...]
      at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775)
      at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
      at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
      at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
      at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
```

> 在 org.apache.spark.repl.Main.$main 中又初始化了 SparkILoop （初始化 SparkContext）

```scala
// org.apache.spark.repl.Main.scala
def main(args: Array[String]) {
  doMain(args, new SparkILoop)
}
```

> 在 org.apache.spark.repl.Main.$doMain 函数启动了 REPL

```scala
// org.apache.spark.repl.Main.scala
private[repl] def doMain(args: Array[String], _interp: SparkILoop): Unit = {
  interp = _interp
  [...]
  if (!hasErrors) {
    interp.process(settings) // Repl starts and goes in loop of R.E.P.L
    [...]
  }
}
```

> org.apache.spark.repl.SparkILoop 继承了 ILoop 类，同时在重写 loadFiles 函数时调用了 initializeSpark 函数，从而初始化了 SparkContext 对象。

```scala
// org.apache.spark.repl.SparkILoop.scala
/**
 * We override `loadFiles` because we need to initialize Spark *before* the REPL
 * sees any files, so that the Spark context is visible in those files. This is a bit of a
 * hack, but there isn't another hook available to us at this point.
 */
override def loadFiles(settings: Settings): Unit = {
  initializeSpark()
  [...]
}

def initializeSpark() {
  intp.beQuietDuring {
    processLine("""
      @transient val spark = if (org.apache.spark.repl.Main.sparkSession != null) {
          org.apache.spark.repl.Main.sparkSession
        } else {
          org.apache.spark.repl.Main.createSparkSession()
        }
      [...]
    """)
  }
}
```

```scala
// org.apache.spark.repl.Main.scala
def createSparkSession(): SparkSession = {
  [...]
  val builder = SparkSession.builder.config(conf)
  if (conf.get(CATALOG_IMPLEMENTATION.key, "hive").toLowerCase(Locale.ROOT) == "hive") {
    if (SparkSession.hiveClassesArePresent) {
      // In the case that the property is not set at all, builder's config
      // does not have this value set to 'hive' yet. The original default
      // behavior is that when there are hive classes, we use hive catalog.
      sparkSession = builder.enableHiveSupport().getOrCreate()
      logInfo("Created Spark session with Hive support")
    } else {
      // Need to change it back to 'in-memory' if no hive classes are found
      // in the case that the property is set to hive in spark-defaults.conf
      builder.config(CATALOG_IMPLEMENTATION.key, "in-memory")
      sparkSession = builder.getOrCreate()
      logInfo("Created Spark session")
    }
  } else {
    // In the case that the property is set but not to 'hive', the internal
    // default is 'in-memory'. So the sparkSession will use in-memory catalog.
    sparkSession = builder.getOrCreate()
    logInfo("Created Spark session")
  }
  sparkContext = sparkSession.sparkContext
  sparkSession
}
```

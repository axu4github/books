# 5.sparkenv

## 5.1 上下文

在 `SparkContext` 初始化的时候，涉及 `SparkEnv` 的有如下代码。

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/SparkContext.scala

class SparkContext(config: SparkConf) extends Logging {
    // Create the Spark execution environment (cache, map output tracker, etc)
    // def isLocal: Boolean = Utils.isLocalMaster(_conf)
    _env = createSparkEnv(_conf, isLocal, listenerBus)
    SparkEnv.set(_env)
    
    // If running the REPL, register the repl's output dir with the file server.
    _conf.getOption("spark.repl.class.outputDir").foreach { path =>
        val replUri = _env.rpcEnv.fileServer.addDirectory("/classes", new File(path))
        _conf.set("spark.repl.class.uri", replUri)
    }
    
    _ui =
        if (conf.getBoolean("spark.ui.enabled", true)) {
            Some(SparkUI.createLiveUI(this, _conf, listenerBus, _jobProgressListener,
                _env.securityManager, appName, startTime = startTime))
        } else {
            // For tests, do not enable the UI
            None
        }
        
    _env.blockManager.initialize(_applicationId)
    
    // The metrics system for Driver need to be set spark.app.id to app ID.
    // So it should start after we get app ID from the task scheduler and set spark.app.id.
    _env.metricsSystem.start()
    // Attach the driver metrics servlet handler to the web ui after the metrics system is started.
    _env.metricsSystem.getServletHandlers.foreach(handler => ui.foreach(_.attachHandler(handler)))
    
    _env.metricsSystem.registerSource(_dagScheduler.metricsSource)
    _env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager))
    _executorAllocationManager.foreach { e =>
        _env.metricsSystem.registerSource(e.executorAllocationManagerSource)
    }
}
```

## 5.2 createSparkEnv()

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/SparkContext.scala

// This function allows components created by SparkEnv to be mocked in unit tests:
private[spark] def createSparkEnv(
    conf: SparkConf,
    isLocal: Boolean,
    listenerBus: LiveListenerBus): SparkEnv = {
    SparkEnv.createDriverEnv(conf, isLocal, listenerBus, SparkContext.numDriverCores(master))
}

/**
* The number of driver cores to use for execution in local mode, 0 otherwise.
*/
private[spark] def numDriverCores(master: String): Int = {
    def convertToInt(threads: String): Int = {
        // 如果未设置执行线程数的话，则使用现有系统所有可用的内核
        if (threads == "*") Runtime.getRuntime.availableProcessors() else threads.toInt
    }
    
    // 这里都是不同 Local 模式下的内核数量获取
    master match {
        case "local" => 1
        case SparkMasterRegex.LOCAL_N_REGEX(threads) => convertToInt(threads)
        case SparkMasterRegex.LOCAL_N_FAILURES_REGEX(threads, _) => convertToInt(threads)
        case _ => 0 // driver is not used for execution
    }
}
```

## 5.3 SparkEnv.createDriverEnv()

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/SparkEnv.scala

object SparkEnv extends Logging {
    @volatile private var env: SparkEnv = _
    
    private[spark] val driverSystemName = "sparkDriver"
    private[spark] val executorSystemName = "sparkExecutor"
    
    /**
    * Create a SparkEnv for the driver.
    */
    private[spark] def createDriverEnv(
        conf: SparkConf,
        isLocal: Boolean,
        listenerBus: LiveListenerBus,
        numCores: Int,
        mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv = {

        assert(conf.contains(DRIVER_HOST_ADDRESS),
            s"${DRIVER_HOST_ADDRESS.key} is not set on the driver!")
        assert(conf.contains("spark.driver.port"), "spark.driver.port is not set on the driver!")
        
        // 获取创建 Driver 的基本信息
        val bindAddress = conf.get(DRIVER_BIND_ADDRESS)
        val advertiseAddress = conf.get(DRIVER_HOST_ADDRESS)
        val port = conf.get("spark.driver.port").toInt
        val ioEncryptionKey = if (conf.get(IO_ENCRYPTION_ENABLED)) {
            Some(CryptoStreamUtils.createKey(conf))
        } else {
            None
        }
        
        // 调用 create() 方法初始化 SparkEnv
        create(
            conf,
            SparkContext.DRIVER_IDENTIFIER,
            bindAddress,
            advertiseAddress,
            port,
            isLocal,
            numCores,
            ioEncryptionKey,
            listenerBus = listenerBus,
            mockOutputCommitCoordinator = mockOutputCommitCoordinator
        )
    }
}

```

## 5.4 SparkEnv.create() 

**重要！！！在 create() 函数中把需要初始化的内容全部初始化完成。**

| 类名 | 作用 |
| --- | --- |
| SecurityManager | 安全管理 |
| SerializerManager | 序列化管理 |
| BroadcastManager | 广播管理 |
| mapOutputTracker | 用来缓存 MapStatus 信息，并提供从 MapOutputMaster 获取信息的功能 |
| ShufflelManager | 路由维护表 |
| MemoryManager | 内存管理 |
| BlockManager | 数据块管理 |
| MetricsSystem | 测量系统 |
| outputCommitCoordinator | 输出协调器 |

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/SparkEnv.scala

/**
* Helper method to create a SparkEnv for a driver or an executor.
*/
private def create(
    conf: SparkConf,
    executorId: String,
    bindAddress: String,
    advertiseAddress: String,
    port: Int,
    isLocal: Boolean,
    numUsableCores: Int,
    ioEncryptionKey: Option[Array[Byte]],
    listenerBus: LiveListenerBus = null,
    mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv = {

    // private[spark] val DRIVER_IDENTIFIER = "driver"
    val isDriver = executorId == SparkContext.DRIVER_IDENTIFIER

    // Listener bus is only used on the driver
    if (isDriver) {
        assert(listenerBus != null, "Attempted to create driver SparkEnv with null listener bus!")
    }

    // 1.SecurityManager -> 安全管理。
    val securityManager = new SecurityManager(conf, ioEncryptionKey)
    ioEncryptionKey.foreach { _ =>
        if (!securityManager.isEncryptionEnabled()) {
            logWarning("I/O encryption enabled without RPC encryption: keys will be visible on the " +
                "wire.")
        }
    }

    // private[spark] val driverSystemName = "sparkDriver"
    // private[spark] val executorSystemName = "sparkExecutor"
    val systemName = if (isDriver) driverSystemName else executorSystemName
    
    // 最后会使用 NettyRpcEnvFactory
    // val config = RpcEnvConfig(conf, name, bindAddress, advertiseAddress, port, securityManager, clientMode)
    // new NettyRpcEnvFactory().create(config)
    val rpcEnv = RpcEnv.create(systemName, bindAddress, advertiseAddress, port, conf,
        securityManager, clientMode = !isDriver)

    // Figure out which port RpcEnv actually bound to in case the original port is 0 or occupied.
    // In the non-driver case, the RPC env's address may be null since it may not be listening
    // for incoming connections.
    if (isDriver) {
        conf.set("spark.driver.port", rpcEnv.address.port.toString)
    } else if (rpcEnv.address != null) {
        conf.set("spark.executor.port", rpcEnv.address.port.toString)
        logInfo(s"Setting spark.executor.port to: ${rpcEnv.address.port.toString}")
    }

    // Create an instance of the class with the given name, possibly initializing it with our conf
    // 根据名称初始化对应的类
    def instantiateClass[T](className: String): T = {
        val cls = Utils.classForName(className)
        // Look for a constructor taking a SparkConf and a boolean isDriver, then one taking just
        // SparkConf, then one taking no arguments
        try {
            cls.getConstructor(classOf[SparkConf], java.lang.Boolean.TYPE)
                .newInstance(conf, new java.lang.Boolean(isDriver))
                .asInstanceOf[T]
        } catch {
            case _: NoSuchMethodException =>
                try {
                    cls.getConstructor(classOf[SparkConf]).newInstance(conf).asInstanceOf[T]
                } catch {
                    case _: NoSuchMethodException =>
                        cls.getConstructor().newInstance().asInstanceOf[T]
                }
        }
    }

    // Create an instance of the class named by the given SparkConf property, or defaultClassName
    // if the property is not set, possibly initializing it with our conf
    def instantiateClassFromConf[T](propertyName: String, defaultClassName: String): T = {
        instantiateClass[T](conf.get(propertyName, defaultClassName))
    }

    // 初始化 Serializer 类，默认使用 JavaSerializer。
    val serializer = instantiateClassFromConf[Serializer](
        "spark.serializer", "org.apache.spark.serializer.JavaSerializer")
    logDebug(s"Using serializer: ${serializer.getClass}")
    
    // 2.SerializerManager -> 序列化管理。
    val serializerManager = new SerializerManager(serializer, conf, ioEncryptionKey)
    val closureSerializer = new JavaSerializer(conf)

    def registerOrLookupEndpoint(
        name: String, endpointCreator: => RpcEndpoint):
        RpcEndpointRef = {
        if (isDriver) {
            logInfo("Registering " + name)
            rpcEnv.setupEndpoint(name, endpointCreator)
        } else {
            RpcUtils.makeDriverRef(name, conf, rpcEnv)
        }
    }

    // 3.BroadcastManager -> 广播管理。
    // 会使用 TorrentBroadcastFactory 
    val broadcastManager = new BroadcastManager(isDriver, conf, securityManager)

    // 4.mapOutputTracker -> 用来缓存 MapStatus 信息，并提供从 MapOutputMaster 获取信息的功能。
    val mapOutputTracker = if (isDriver) {
        new MapOutputTrackerMaster(conf, broadcastManager, isLocal)
    } else {
        new MapOutputTrackerWorker(conf)
    }

    // Have to assign trackerEndpoint after initialization as MapOutputTrackerEndpoint
    // requires the MapOutputTracker itself
    mapOutputTracker.trackerEndpoint = registerOrLookupEndpoint(MapOutputTracker.ENDPOINT_NAME,
        new MapOutputTrackerMasterEndpoint(
            rpcEnv, mapOutputTracker.asInstanceOf[MapOutputTrackerMaster], conf))

    // Let the user specify short names for shuffle managers
    // 5.ShufflelManager -> 路由维护表。
    val shortShuffleMgrNames = Map(
        "sort" -> classOf[org.apache.spark.shuffle.sort.SortShuffleManager].getName,
        "tungsten-sort" -> classOf[org.apache.spark.shuffle.sort.SortShuffleManager].getName)
    val shuffleMgrName = conf.get("spark.shuffle.manager", "sort")
    val shuffleMgrClass =
        shortShuffleMgrNames.getOrElse(shuffleMgrName.toLowerCase(Locale.ROOT), shuffleMgrName)
    val shuffleManager = instantiateClass[ShuffleManager](shuffleMgrClass)

    // 6.MemoryManager -> 内存管理。
    val useLegacyMemoryManager = conf.getBoolean("spark.memory.useLegacyMode", false)
    val memoryManager: MemoryManager =
        if (useLegacyMemoryManager) {
            new StaticMemoryManager(conf, numUsableCores)
        } else {
            UnifiedMemoryManager(conf, numUsableCores)
        }

    // 7.BlockManager -> 数据块管理。
    val blockManagerPort = if (isDriver) {
        conf.get(DRIVER_BLOCK_MANAGER_PORT)
    } else {
        conf.get(BLOCK_MANAGER_PORT)
    }
    val blockTransferService =
        new NettyBlockTransferService(conf, securityManager, bindAddress, advertiseAddress,
            blockManagerPort, numUsableCores)
    val blockManagerMaster = new BlockManagerMaster(registerOrLookupEndpoint(
        BlockManagerMaster.DRIVER_ENDPOINT_NAME,
        new BlockManagerMasterEndpoint(rpcEnv, isLocal, conf, listenerBus)),
conf, isDriver)
    // NB: blockManager is not valid until initialize() is called later.
    val blockManager = new BlockManager(executorId, rpcEnv, blockManagerMaster,
        serializerManager, conf, memoryManager, mapOutputTracker, shuffleManager,
        blockTransferService, securityManager, numUsableCores)

    // 8.MetricsSystem -> 测量系统。
    val metricsSystem = if (isDriver) {
        // Don't start metrics system right now for Driver.
        // We need to wait for the task scheduler to give us an app ID.
        // Then we can start the metrics system.
        MetricsSystem.createMetricsSystem("driver", conf, securityManager)
    } else {
        // We need to set the executor ID before the MetricsSystem is created because sources and
        // sinks specified in the metrics configuration file will want to incorporate this executor's
        // ID into the metrics they report.
        conf.set("spark.executor.id", executorId)
        val ms = MetricsSystem.createMetricsSystem("executor", conf, securityManager)
        ms.start()
        ms
    }

    // 9.outputCommitCoordinator -> 输出协调器。
    val outputCommitCoordinator = mockOutputCommitCoordinator.getOrElse {
        new OutputCommitCoordinator(conf, isDriver)
    }
    val outputCommitCoordinatorRef = registerOrLookupEndpoint("OutputCommitCoordinator",
    new OutputCommitCoordinatorEndpoint(rpcEnv, outputCommitCoordinator))
        outputCommitCoordinator.coordinatorRef = Some(outputCommitCoordinatorRef)

    // 初始化 SparkEnv 类
    val envInstance = new SparkEnv(
        executorId,
        rpcEnv,
        serializer,
        closureSerializer,
        serializerManager,
        mapOutputTracker,
        shuffleManager,
        broadcastManager,
        blockManager,
        securityManager,
        metricsSystem,
        memoryManager,
        outputCommitCoordinator,
        conf)

    // Add a reference to tmp dir created by driver, we will delete this tmp dir when stop() is
    // called, and we only need to do it for driver. Because driver may run as a service, and if we
    // don't delete this tmp dir when sc is stopped, then will create too many tmp dirs.
    if (isDriver) {
        val sparkFilesDir = Utils.createTempDir(Utils.getLocalDir(conf), "userFiles").getAbsolutePath
        envInstance.driverTmpDir = Some(sparkFilesDir)
    }
    
    envInstance
}
```

## 5.5 SparkEnv()

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/SparkEnv.scala

@DeveloperApi
class SparkEnv (
    val executorId: String,
    private[spark] val rpcEnv: RpcEnv,
    val serializer: Serializer,
    val closureSerializer: Serializer,
    val serializerManager: SerializerManager,
    val mapOutputTracker: MapOutputTracker,
    val shuffleManager: ShuffleManager,
    val broadcastManager: BroadcastManager,
    val blockManager: BlockManager,
    val securityManager: SecurityManager,
    val metricsSystem: MetricsSystem,
    val memoryManager: MemoryManager,
    val outputCommitCoordinator: OutputCommitCoordinator,
    val conf: SparkConf) extends Logging {
    
    private[spark] var isStopped = false
    private val pythonWorkers = mutable.HashMap[(String, Map[String, String]), PythonWorkerFactory]()
    
    // A general, soft-reference map for metadata needed during HadoopRDD split computation
    // (e.g., HadoopFileRDD uses this to cache JobConfs and InputFormats).
    private[spark] val hadoopJobMetadata = new MapMaker().softValues().makeMap[String, Any]()
    
    private[spark] var driverTmpDir: Option[String] = None
}
```


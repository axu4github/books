# 6.SecurityManager

> 在 `SparkEnv` 中初始化了几个重要的类，从现在这节开始一次看看这些类的作用以及实现方式，第一个就是 `SecurityManager`。

## 6.1 上下文

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/SparkEnv.scala

private def create(
    conf: SparkConf,
    executorId: String,
    bindAddress: String,
    advertiseAddress: String,
    port: Int,
    isLocal: Boolean,
    numUsableCores: Int,
    ioEncryptionKey: Option[Array[Byte]],
    listenerBus: LiveListenerBus = null,
    mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv = {
    // 1.SecurityManager -> 安全管理。
    val securityManager = new SecurityManager(conf, ioEncryptionKey)
    ioEncryptionKey.foreach { _ =>
        if (!securityManager.isEncryptionEnabled()) {
            logWarning("I/O encryption enabled without RPC encryption: keys will be visible on the " +
                "wire.")
        }
    }
    
    // 最后会使用 NettyRpcEnvFactory
    // val config = RpcEnvConfig(conf, name, bindAddress, advertiseAddress, port, securityManager, clientMode)
    // new NettyRpcEnvFactory().create(config)
    val rpcEnv = RpcEnv.create(systemName, bindAddress, advertiseAddress, port, conf,
        securityManager, clientMode = !isDriver)
        
    // 3.BroadcastManager -> 广播管理。
    // 会使用 TorrentBroadcastFactory 
    val broadcastManager = new BroadcastManager(isDriver, conf, securityManager)
    
    val blockTransferService =
        new NettyBlockTransferService(conf, securityManager, bindAddress, advertiseAddress,
            blockManagerPort, numUsableCores)
            
    // NB: blockManager is not valid until initialize() is called later.
    val blockManager = new BlockManager(executorId, rpcEnv, blockManagerMaster,
        serializerManager, conf, memoryManager, mapOutputTracker, shuffleManager,
        blockTransferService, securityManager, numUsableCores)
        
     // 8.MetricsSystem -> 测量系统。
    val metricsSystem = if (isDriver) {
        // Don't start metrics system right now for Driver.
        // We need to wait for the task scheduler to give us an app ID.
        // Then we can start the metrics system.
        MetricsSystem.createMetricsSystem("driver", conf, securityManager)
    } else {
        // We need to set the executor ID before the MetricsSystem is created because sources and
        // sinks specified in the metrics configuration file will want to incorporate this executor's
        // ID into the metrics they report.
        conf.set("spark.executor.id", executorId)
        val ms = MetricsSystem.createMetricsSystem("executor", conf, securityManager)
        ms.start()
        ms
    }
    
    // 初始化 SparkEnv 类
    val envInstance = new SparkEnv(
        executorId,
        rpcEnv,
        serializer,
        closureSerializer,
        serializerManager,
        mapOutputTracker,
        shuffleManager,
        broadcastManager,
        blockManager,
        securityManager,
        metricsSystem,
        memoryManager,
        outputCommitCoordinator,
        conf)
}
        
```


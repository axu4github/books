# 4.sparklistenerbus

## 4.1 上下文

在 `SparkContext` 初始化的时候，涉及 `SparkLisnerBus` 的有如下代码。（**注意：这里只是贴出和 SparkLisnerBus 相关的代码，其他代码可在 SparkContext 章节查看。**）

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/SparkContext.scala

class SparkContext(config: SparkConf) extends Logging {
  private var _jobProgressListener: JobProgressListener = _
  private var _listenerBusStarted: Boolean = false
    
  // An asynchronous listener bus for Spark events
  private[spark] val listenerBus = new LiveListenerBus(this)
  
  private[spark] def jobProgressListener: JobProgressListener = _jobProgressListener
  
  try {
    // "_jobProgressListener" should be set up before creating SparkEnv because when creating
    // "SparkEnv", some messages will be posted to "listenerBus" and we should not miss them.
    // JobProgressListener 初始化就是声明了一系列 Job 相关的变量。
    // 另外在 spark 2.2.0 版本中将取消该类。
    _jobProgressListener = new JobProgressListener(_conf)
    listenerBus.addListener(jobProgressListener)
    
    // #?# 待看完 SparkEnv 部分以后补充
    _env = createSparkEnv(_conf, isLocal, listenerBus)
    
    // #?# 待看完 ui 部分以后补充
    _ui =
      if (conf.getBoolean("spark.ui.enabled", true)) {
        Some(SparkUI.createLiveUI(this, _conf, listenerBus, _jobProgressListener,
          _env.securityManager, appName, startTime = startTime))
      } else {
        // For tests, do not enable the UI
        None
      }
      
    // private[spark] def isEventLogEnabled: Boolean = _conf.getBoolean("spark.eventLog.enabled", false)
    _eventLogger =
      if (isEventLogEnabled) {
        val logger =
        new EventLoggingListener(_applicationId, _applicationAttemptId, _eventLogDir.get,
          _conf, _hadoopConfiguration)
        logger.start()
        listenerBus.addListener(logger)
        Some(logger)
      } else {
        None
      }
      
    // Optionally scale number of executors dynamically based on workload. Exposed for testing.
    val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf)
    _executorAllocationManager =
      if (dynamicAllocationEnabled) {
        schedulerBackend match {
        case b: ExecutorAllocationClient =>
          Some(new ExecutorAllocationManager(
            schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf))
        case _ =>
          None
        }
      } else {
        None
      }
    _executorAllocationManager.foreach(_.start())
       
    // 重要，最后调用了 listenerBus.start() 方法，启动 Listeners。
    setupAndStartListenerBus()
    
    // 向已经启动的 Listeners，发送执事件（post event）。
    postEnvironmentUpdate()
    postApplicationStart()
  } catch {
    [...]
  }
  
  /**
  * Registers listeners specified in spark.extraListeners, then starts the listener bus.
  * This should be called after all internal listeners have been registered with the listener bus
  * (e.g. after the web UI and event logging listeners have been registered).
  */
  private def setupAndStartListenerBus(): Unit = {
    // Use reflection to instantiate listeners specified via `spark.extraListeners`
    try {
      val listenerClassNames: Seq[String] =
        conf.get("spark.extraListeners", "").split(',').map(_.trim).filter(_ != "")
      // 如果配置中设置了 extraListeners 则根据配置注册。 
      for (className <- listenerClassNames) {
        [...]
        listenerBus.addListener(listener)
        logInfo(s"Registered listener $className")
      }
    } catch {
      [...]
    }
  
    // 最重要的是在这里调用 start 函数启动 listenerBus。
    listenerBus.start()
    _listenerBusStarted = true
  }
}

```

## 4.2 LiveListenerBus 初始化

进入 `LiveListenerBus` 类，看下在声明时都初始化了什么变量。

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala

// 继承关系 ListenerBus -> SparkListenerBus -> LiveListenerBus
private[spark] class LiveListenerBus(val sparkContext: SparkContext) extends SparkListenerBus {
	self =>
	
	// 导入 LiveListenerBus 伴生对象中声明的变量
	import LiveListenerBus._
	
	// Atomic${Type} 类声明的变量是为了在多线程运行时，在当前线程执行完成后，其他线程才可以继续修改该值。
	// Atomic${Type} 类这些类声明在 java.util.concurrent.atomic 包中，一共有四个分别是：AtomicBoolean, AtomicInteger, AtomicLong, AtomicReference。
	// Indicate if `start()` is called
	private val started = new AtomicBoolean(false)
	// Indicate if `stop()` is called
	private val stopped = new AtomicBoolean(false)
	
	/** A counter for dropped events. It will be reset every time we log it. */
	private val droppedEventsCounter = new AtomicLong(0L)
	
	/** When `droppedEventsCounter` was logged last time in milliseconds. */
	// #?# 修饰符 volatile、Atomic${Type} 和 synchronized 
	@volatile private var lastReportTimestamp = 0L
	
	// Indicate if we are processing some event
	// Guarded by `self`
	private var processingEvent = false
	
	private val logDroppedEvent = new AtomicBoolean(false)
	
	// A counter that represents the number of events produced and consumed in the queue
	private val eventLock = new Semaphore(0)
}

// LiveListenerBus 类伴生对象
private[spark] object LiveListenerBus {
	// Allows for Context to check whether stop() call is made within listener thread
	val withinListenerThread: DynamicVariable[Boolean] = new DynamicVariable[Boolean](false)
	
	/** The thread name of Spark listener bus */
	val name = "SparkListenerBus"
}
```

看到初始化 `LiveListenerBus` 类时，会初始化其父类，在上段代码中可以看到继承关系 `ListenerBus -> SparkListenerBus -> LiveListenerBus`。
`SparkListenerBus` 类没有初始化内容，直接看 `ListenerBus` 类。

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/util/ListenerBus.scala

private[spark] trait ListenerBus[L <: AnyRef, E] extends Logging {

	// Marked `private[spark]` for access in tests.
	// CopyOnWriteArrayList 类似一个不可变的数组，每一次添加或者修改都意味着会创建一个新的数组，和 RDD 很像。
	private[spark] val listeners = new CopyOnWriteArrayList[L]
  
}
```

**总结：LiveListenerBus 实际是初始化了一个 listeners。**

## 4.3 LiveListenerBus.start()

### 4.3.1 LiveListenerBus.start()

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala

/**
* Start sending events to attached listeners.
*
* This first sends out all buffered events posted before this listener bus has started, then
* listens for any additional events asynchronously while the listener bus is still running.
* This should only be called once.
*
*/
def start(): Unit = {
    if (started.compareAndSet(false, true)) {
        listenerThread.start()
    } else {
        throw new IllegalStateException(s"$name already started!")
    }
}

private lazy val EVENT_QUEUE_CAPACITY = validateAndGetQueueSize()
// 使用阻塞队列 LinkedBlockingQueue
private lazy val eventQueue = new LinkedBlockingQueue[SparkListenerEvent](EVENT_QUEUE_CAPACITY)

private def validateAndGetQueueSize(): Int = {
    // 默认 10000
    // ${SPARK_HOME}/core/src/main/scala/org/apache/spark/internal/config/package.scala 中设置
    val queueSize = sparkContext.conf.get(LISTENER_BUS_EVENT_QUEUE_SIZE)
    if (queueSize <= 0) {
        throw new SparkException("spark.scheduler.listenerbus.eventqueue.size must be > 0!")
    }
    queueSize
}

// 如果 event 不为空
private val listenerThread = new Thread(name) {
    setDaemon(true)
    override def run(): Unit = Utils.tryOrStopSparkContext(sparkContext) {
        LiveListenerBus.withinListenerThread.withValue(true) {
            while (true) {
                eventLock.acquire()
                self.synchronized {
                    processingEvent = true
                }
                try {
                    val event = eventQueue.poll
                    if (event == null) {
                        // Get out of the while loop and shutdown the daemon thread
                        if (!stopped.get) {
                            throw new IllegalStateException("Polling `null` from eventQueue means" +
                    " the listener bus has been stopped. So `stopped` must be true")
                        }
                        return
                    }
                    // 这里调用了父父类 ListenerBus.postToAll() 方法。
                    postToAll(event)
                } finally {
                    self.synchronized {
                        processingEvent = false
                    }
                }
            }
        }
    }
}
```

### 4.3.2 父父类 ListenerBus.postToAll()

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/util/ListenerBus.scala

/**
* Post the event to all registered listeners. The `postToAll` caller should guarantee calling
* `postToAll` in the same thread for all events.
*/
def postToAll(event: E): Unit = {
    // JavaConverters can create a JIterableWrapper if we use asScala.
    // However, this method will be called frequently. To avoid the wrapper cost, here we use
    // Java Iterator directly.
    val iter = listeners.iterator
    while (iter.hasNext) {
        val listener = iter.next()
        try {
            // doPostEvent 在这里是个抽象方法，必须在继承的子类中实现
            // 这里调用父类 SparkListenerBus.doPostEvent() 方法
            doPostEvent(listener, event)
        } catch {
            case NonFatal(e) =>
                logError(s"Listener ${Utils.getFormattedClassName(listener)} threw an exception", e)
        }
    }
}

/**
* Post an event to the specified listener. `onPostEvent` is guaranteed to be called in the same
* thread for all listeners.
*/
protected def doPostEvent(listener: L, event: E): Unit
```

### 4.3.3 父类 SparkListenerBus.doPostEvent()

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala

// 这里复写了 SparkListenerBus 父类 ListenerBus 的 doPostEvent 方法。
protected override def doPostEvent(
    listener: SparkListenerInterface,
    event: SparkListenerEvent): Unit = {
    // 根据 listener 和 event 去执行不同的方法
    event match {
        case stageSubmitted: SparkListenerStageSubmitted =>
            listener.onStageSubmitted(stageSubmitted)
        case stageCompleted: SparkListenerStageCompleted =>
            listener.onStageCompleted(stageCompleted)
        case jobStart: SparkListenerJobStart =>
            listener.onJobStart(jobStart)
        case jobEnd: SparkListenerJobEnd =>
            listener.onJobEnd(jobEnd)
        case taskStart: SparkListenerTaskStart =>
            listener.onTaskStart(taskStart)
        case taskGettingResult: SparkListenerTaskGettingResult =>
            listener.onTaskGettingResult(taskGettingResult)
        case taskEnd: SparkListenerTaskEnd =>
            listener.onTaskEnd(taskEnd)
        case environmentUpdate: SparkListenerEnvironmentUpdate =>
            listener.onEnvironmentUpdate(environmentUpdate)
        case blockManagerAdded: SparkListenerBlockManagerAdded =>
            listener.onBlockManagerAdded(blockManagerAdded)
        case blockManagerRemoved: SparkListenerBlockManagerRemoved =>
            listener.onBlockManagerRemoved(blockManagerRemoved)
        case unpersistRDD: SparkListenerUnpersistRDD =>
            listener.onUnpersistRDD(unpersistRDD)
        case applicationStart: SparkListenerApplicationStart =>
            listener.onApplicationStart(applicationStart)
        case applicationEnd: SparkListenerApplicationEnd =>
            listener.onApplicationEnd(applicationEnd)
        case metricsUpdate: SparkListenerExecutorMetricsUpdate =>
            listener.onExecutorMetricsUpdate(metricsUpdate)
        case executorAdded: SparkListenerExecutorAdded =>
            listener.onExecutorAdded(executorAdded)
        case executorRemoved: SparkListenerExecutorRemoved =>
            listener.onExecutorRemoved(executorRemoved)
        case executorBlacklisted: SparkListenerExecutorBlacklisted =>
            listener.onExecutorBlacklisted(executorBlacklisted)
        case executorUnblacklisted: SparkListenerExecutorUnblacklisted =>
            listener.onExecutorUnblacklisted(executorUnblacklisted)
        case nodeBlacklisted: SparkListenerNodeBlacklisted =>
            listener.onNodeBlacklisted(nodeBlacklisted)
        case nodeUnblacklisted: SparkListenerNodeUnblacklisted =>
            listener.onNodeUnblacklisted(nodeUnblacklisted)
        case blockUpdated: SparkListenerBlockUpdated =>
            listener.onBlockUpdated(blockUpdated)
        case logStart: SparkListenerLogStart => // ignore event log metadata
        case _ => listener.onOtherEvent(event)
    }
}
```





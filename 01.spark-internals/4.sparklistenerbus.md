# 4.sparklistenerbus

## 4.1 上下文

在 `SparkContext` 初始化的时候，涉及 `SparkLisnerBus` 的有如下代码。（**注意：这里只是贴出和 SparkLisnerBus 相关的代码，其他代码可在 SparkContext 章节查看。**）

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/SparkContext.scala

class SparkContext(config: SparkConf) extends Logging {
   private var _jobProgressListener: JobProgressListener = _
   private var _listenerBusStarted: Boolean = false
    
   // An asynchronous listener bus for Spark events
	private[spark] val listenerBus = new LiveListenerBus(this)
	
	private[spark] def jobProgressListener: JobProgressListener = _jobProgressListener
	
	try {
		// "_jobProgressListener" should be set up before creating SparkEnv because when creating
		// "SparkEnv", some messages will be posted to "listenerBus" and we should not miss them.
		_jobProgressListener = new JobProgressListener(_conf)
		listenerBus.addListener(jobProgressListener)
		
		// #?# 待看完 SparkEnv 部分以后补充
		_env = createSparkEnv(_conf, isLocal, listenerBus)
		
		// #?# 待看完 ui 部分以后补充
		_ui =
			if (conf.getBoolean("spark.ui.enabled", true)) {
				Some(SparkUI.createLiveUI(this, _conf, listenerBus, _jobProgressListener,
					_env.securityManager, appName, startTime = startTime))
			} else {
				// For tests, do not enable the UI
				None
			}
			
		// private[spark] def isEventLogEnabled: Boolean = _conf.getBoolean("spark.eventLog.enabled", false)
		_eventLogger =
			if (isEventLogEnabled) {
				val logger =
				new EventLoggingListener(_applicationId, _applicationAttemptId, _eventLogDir.get,
					_conf, _hadoopConfiguration)
				logger.start()
				listenerBus.addListener(logger)
				Some(logger)
			} else {
				None
			}
			
		// Optionally scale number of executors dynamically based on workload. Exposed for testing.
		val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf)
		_executorAllocationManager =
			if (dynamicAllocationEnabled) {
				schedulerBackend match {
				case b: ExecutorAllocationClient =>
					Some(new ExecutorAllocationManager(
						schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf))
				case _ =>
					None
				}
			} else {
				None
			}
		_executorAllocationManager.foreach(_.start())
		
		
		setupAndStartListenerBus()
	} catch {
		[...]
	}
	
	/**
	* Registers listeners specified in spark.extraListeners, then starts the listener bus.
	* This should be called after all internal listeners have been registered with the listener bus
	* (e.g. after the web UI and event logging listeners have been registered).
	*/
	private def setupAndStartListenerBus(): Unit = {
	  // Use reflection to instantiate listeners specified via `spark.extraListeners`
	  try {
	    val listenerClassNames: Seq[String] =
	      conf.get("spark.extraListeners", "").split(',').map(_.trim).filter(_ != "")
	    for (className <- listenerClassNames) {
	      // Use reflection to find the right constructor
	      val constructors = {
	        val listenerClass = Utils.classForName(className)
	        listenerClass
	          .getConstructors
	          .asInstanceOf[Array[Constructor[_ <: SparkListenerInterface]]]
	      }
	      val constructorTakingSparkConf = constructors.find { c =>
	        c.getParameterTypes.sameElements(Array(classOf[SparkConf]))
	      }
	      lazy val zeroArgumentConstructor = constructors.find { c =>
	        c.getParameterTypes.isEmpty
	      }
	      val listener: SparkListenerInterface = {
	        if (constructorTakingSparkConf.isDefined) {
	          constructorTakingSparkConf.get.newInstance(conf)
	        } else if (zeroArgumentConstructor.isDefined) {
	          zeroArgumentConstructor.get.newInstance()
	        } else {
	          throw new SparkException(
	            s"$className did not have a zero-argument constructor or a" +
	              " single-argument constructor that accepts SparkConf. Note: if the class is" +
	              " defined inside of another Scala class, then its constructors may accept an" +
	              " implicit parameter that references the enclosing class; in this case, you must" +
	              " define the listener as a top-level class in order to prevent this extra" +
	              " parameter from breaking Spark's ability to find a valid constructor.")
	        }
	      }
	      listenerBus.addListener(listener)
	      logInfo(s"Registered listener $className")
	    }
	  } catch {
	    case e: Exception =>
	      try {
	        stop()
	      } finally {
	        throw new SparkException(s"Exception when registering SparkListener", e)
	      }
	  }
	
	  listenerBus.start()
	  _listenerBusStarted = true
	}
}

```

## 4.2 LiveListenerBus 初始化

进入 `LiveListenerBus` 类，看下在声明时都初始化了什么变量。

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala

// 继承关系 ListenerBus -> SparkListenerBus -> LiveListenerBus
private[spark] class LiveListenerBus(val sparkContext: SparkContext) extends SparkListenerBus {
	self =>
	
	// 导入 LiveListenerBus 伴生对象中声明的变量
	import LiveListenerBus._
	
	// Atomic${Type} 类声明的变量是为了在多线程运行时，在当前线程执行完成后，其他线程才可以继续修改该值。
	// Atomic${Type} 类这些类声明在 java.util.concurrent.atomic 包中，一共有四个分别是：AtomicBoolean, AtomicInteger, AtomicLong, AtomicReference。
	// Indicate if `start()` is called
	private val started = new AtomicBoolean(false)
	// Indicate if `stop()` is called
	private val stopped = new AtomicBoolean(false)
	
	/** A counter for dropped events. It will be reset every time we log it. */
	private val droppedEventsCounter = new AtomicLong(0L)
	
	/** When `droppedEventsCounter` was logged last time in milliseconds. */
	// 修饰符 volatile、Atomic${Type} 和 
	@volatile private var lastReportTimestamp = 0L
	
	// Indicate if we are processing some event
	// Guarded by `self`
	private var processingEvent = false
	
	private val logDroppedEvent = new AtomicBoolean(false)
	
	// A counter that represents the number of events produced and consumed in the queue
	private val eventLock = new Semaphore(0)
}

// LiveListenerBus 类伴生对象
private[spark] object LiveListenerBus {
	// Allows for Context to check whether stop() call is made within listener thread
	val withinListenerThread: DynamicVariable[Boolean] = new DynamicVariable[Boolean](false)
	
	/** The thread name of Spark listener bus */
	val name = "SparkListenerBus"
}
```

看到初始化 `LiveListenerBus` 类时，会初始化其父类，在上段代码中可以看到继承关系 `ListenerBus -> SparkListenerBus -> LiveListenerBus`。
`SparkListenerBus` 类没有初始化内容，直接看 `ListenerBus` 类。

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/util/ListenerBus.scala

private[spark] trait ListenerBus[L <: AnyRef, E] extends Logging {

	// Marked `private[spark]` for access in tests.
	// CopyOnWriteArrayList 类似一个不可变的数组，每一次添加或者修改都意味着会创建一个新的数组，和 RDD 很像。
	private[spark] val listeners = new CopyOnWriteArrayList[L]
  
}
```






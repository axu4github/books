# 8.serializermanager

> `SerializerManager` 是初始化 `SparkEnv` 第3-5个参数。

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/SparkEnv.scala

class SparkEnv (
    val executorId: String,
    private[spark] val rpcEnv: RpcEnv,
    val serializer: Serializer,
    val closureSerializer: Serializer,
    val serializerManager: SerializerManager,
    val mapOutputTracker: MapOutputTracker,
    val shuffleManager: ShuffleManager,
    val broadcastManager: BroadcastManager,
    val blockManager: BlockManager,
    val securityManager: SecurityManager,
    val metricsSystem: MetricsSystem,
    val memoryManager: MemoryManager,
    val outputCommitCoordinator: OutputCommitCoordinator,
    val conf: SparkConf) extends Logging {
    [...]
}
```

## 8.1 上下文

```scala
// ${SPARK_HOME}/core/src/main/scala/org/apache/spark/SparkEnv.scala

// Create an instance of the class with the given name, possibly initializing it with our conf
// 通过类名，初始化该类（入参为SparkConf）
def instantiateClass[T](className: String): T = {
    val cls = Utils.classForName(className)
    // Look for a constructor taking a SparkConf and a boolean isDriver, then one taking just
    // SparkConf, then one taking no arguments
    try {
        cls.getConstructor(classOf[SparkConf], java.lang.Boolean.TYPE)
            .newInstance(conf, new java.lang.Boolean(isDriver))
            .asInstanceOf[T]
    } catch {
        case _: NoSuchMethodException =>
            try {
                cls.getConstructor(classOf[SparkConf]).newInstance(conf).asInstanceOf[T]
            } catch {
                case _: NoSuchMethodException =>
                cls.getConstructor().newInstance().asInstanceOf[T]
            }
    }
}

// Create an instance of the class named by the given SparkConf property, or defaultClassName
// if the property is not set, possibly initializing it with our conf
def instantiateClassFromConf[T](propertyName: String, defaultClassName: String): T = {
    instantiateClass[T](conf.get(propertyName, defaultClassName))
}

// 如果有 spark.serializer 配置，则使用该配置作为序列化方法。默认为 org.apache.spark.serializer.JavaSerializer
val serializer = instantiateClassFromConf[Serializer](
    "spark.serializer", "org.apache.spark.serializer.JavaSerializer")
logDebug(s"Using serializer: ${serializer.getClass}")

// 在 SerializerManager 中会根据 spark.io.compression.codec 定义压缩方法（默认为lz4）。
val serializerManager = new SerializerManager(serializer, conf, ioEncryptionKey)

val closureSerializer = new JavaSerializer(conf)

// NB: blockManager is not valid until initialize() is called later.
// 到 BlockManager 章节在看
val blockManager = new BlockManager(executorId, rpcEnv, blockManagerMaster,
    serializerManager, conf, memoryManager, mapOutputTracker, shuffleManager,
    blockTransferService, securityManager, numUsableCores)
    
val envInstance = new SparkEnv(
    executorId,
    rpcEnv,
    serializer,
    closureSerializer,
    serializerManager,
    mapOutputTracker,
    shuffleManager,
    broadcastManager,
    blockManager,
    securityManager,
    metricsSystem,
    memoryManager,
    outputCommitCoordinator,
    conf)
```

## 8.2 SerializerManager

```scala
private[spark] class SerializerManager(
    defaultSerializer: Serializer,
    conf: SparkConf,
    encryptionKey: Option[Array[Byte]]) {
    
    private[this] val kryoSerializer = new KryoSerializer(conf)
    
    private[this] val stringClassTag: ClassTag[String] = implicitly[ClassTag[String]]
    private[this] val primitiveAndPrimitiveArrayClassTags: Set[ClassTag[_]] = {
        val primitiveClassTags = Set[ClassTag[_]](
            ClassTag.Boolean,
            ClassTag.Byte,
            ClassTag.Char,
            ClassTag.Double,
            ClassTag.Float,
            ClassTag.Int,
            ClassTag.Long,
            ClassTag.Null,
            ClassTag.Short
        )
        val arrayClassTags = primitiveClassTags.map(_.wrap)
        primitiveClassTags ++ arrayClassTags
    }
    
    // 获取压缩配置
    // Whether to compress broadcast variables that are stored
    private[this] val compressBroadcast = conf.getBoolean("spark.broadcast.compress", true)
    // Whether to compress shuffle output that are stored
    private[this] val compressShuffle = conf.getBoolean("spark.shuffle.compress", true)
    // Whether to compress RDD partitions that are stored serialized
    private[this] val compressRdds = conf.getBoolean("spark.rdd.compress", false)
    // Whether to compress shuffle output temporarily spilled to disk
    private[this] val compressShuffleSpill = conf.getBoolean("spark.shuffle.spill.compress", true)
    
    /* The compression codec to use. Note that the "lazy" val is necessary because we want to delay
    * the initialization of the compression codec until it is first used. The reason is that a Spark
    * program could be using a user-defined codec in a third party jar, which is loaded in
    * Executor.updateDependencies. When the BlockManager is initialized, user level jars hasn't been
    * loaded yet. */
    // 获取压缩算法，默认为 lz4（在 CompressionCodec 类 DEFAULT_COMPRESSION_CODEC 配置）。
    private lazy val compressionCodec: CompressionCodec = CompressionCodec.createCodec(conf)
    
    def encryptionEnabled: Boolean = encryptionKey.isDefined
}
```

## 8.3 总结

> `SerializerManager` 比较好理解主要就是序列化和反序列化的东西。
> 序列化方法默认使用 `spark` 自己实现的 `JavaSerializer`。
> 压缩算法默认是用 `lz4`。

`-EOF-`



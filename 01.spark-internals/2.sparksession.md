# 2. SparkSession

> 上文提到在执行 `spark-shell` 时，初始化 `SparkSession` 最后是调用 `createSparkSession` 函数完成的。

```scala
// ${SPARK_HOME}/repl/scala-2.11/src/main/scala/org/apache/spark/repl/Main.scala
[...]
import org.apache.spark.sql.SparkSession
[...]

def createSparkSession(): SparkSession = {
  [...]
  val builder = SparkSession.builder.config(conf)
  [...]
  sparkSession = builder.getOrCreate()
  [...]
  sparkSession
}
```

> 本节就从 `createSparkSession` 函数开始进一步探讨初始化 `SparkSession` 的过程。

> 创建 `builder` 的过程是通过 `SparkSession` 的 `伴生对象` 创建。

```scala
// ${SPARK_HOME}/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala

[...]
@InterfaceStability.Stable
object SparkSession {
  [...]
  @InterfaceStability.Stable
  class Builder extends Logging {
    [...]
    def config(conf: SparkConf): Builder = synchronized {
      conf.getAll.foreach { case (k, v) => options += k -> v }
      this
    }
  }

  [...]
  def builder(): Builder = new Builder
}
```

> `builder.getOrCreate()` 的过程，主要调用 `getOrCreate` 函数。

`get` 的过程分为两步：

1. 判断当前线程是否存在一个可用的session。
2. 判断全局中是否存在一个可用的session。

> 其中 `options` 是一个可变的（`mutable`）的 `HashMap`，用来存储所有读进来的配置信息。

```scala
// ${SPARK_HOME}/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala

[...]
@InterfaceStability.Stable
object SparkSession {
  [...]
  @InterfaceStability.Stable
  class Builder extends Logging {
    [...]
    def getOrCreate(): SparkSession = synchronized {
      // Get the session from current thread's active session.
      // 判断当前线程是否存在一个可用的session，如果存在则返回该session。
      var session = activeThreadSession.get()
      if ((session ne null) && !session.sparkContext.isStopped) {
        options.foreach { case (k, v) => session.sessionState.conf.setConfString(k, v) }
        if (options.nonEmpty) {
          logWarning("Using an existing SparkSession; some configuration may not take effect.")
        }
        return session
      }

      // Global synchronization so we will only set the default session once.
      // 判断全局中是否存在一个可用的session，如果存在则返回该session。
      SparkSession.synchronized {
        // If the current thread does not have an active session, get it from the global session.
        session = defaultSession.get()
        if ((session ne null) && !session.sparkContext.isStopped) {
          options.foreach { case (k, v) => session.sessionState.conf.setConfString(k, v) }
          if (options.nonEmpty) {
            logWarning("Using an existing SparkSession; some configuration may not take effect.")
          }
          return session
        }
        [...]
      }
      [...]
    }
  }
}
```

`create` 的过程分为四步：

1. 初始化 `SparkContext`。
2. 如果用户自定义 `SparkSession` 的扩展（`SPARK_SESSION_EXTENSIONS`），则初始化扩展。
3. 将 `SparkContext` 和 `SPARK_SESSION_EXTENSIONS` 作为入参，初始化 `SparkSession`。
4. 设置一个 `SparkListener` 保证在应用结束时将全局的 `session` 置空。

```scala
// ${SPARK_HOME}/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala

[...]
@InterfaceStability.Stable
object SparkSession {
  [...]
  @InterfaceStability.Stable
  class Builder extends Logging {
    [...]
    def getOrCreate(): SparkSession = synchronized {
      [...]
      SparkSession.synchronized {
        // 初始化 `SparkContext`。
        val sparkContext = userSuppliedContext.getOrElse { [...] }

        [...]
        // 如果用户自定义 `SparkSession` 的扩展（`SPARK_SESSION_EXTENSIONS`），则初始化扩展。
        if (extensionConfOption.isDefined) {
          [...]
          extensionConf(extensions)
        }

        // 将 `SparkContext` 和 `SPARK_SESSION_EXTENSIONS` 作为入参，初始化 `SparkSession`。
        session = new SparkSession(sparkContext, None, None, extensions)
        [...]

        // 设置一个 `SparkListener` 保证在应用结束时将全局的 `session` 置空。
        sparkContext.addSparkListener( [...] )
      }

      return session
    }
  }
}
```

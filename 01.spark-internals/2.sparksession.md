# 2. SparkSession

> 上文提到在执行 `spark-shell` 时，初始化 `SparkSession` 最后是调用 `createSparkSession` 函数完成的。

```scala
// ${SPARK_HOME}/repl/scala-2.11/src/main/scala/org/apache/spark/repl/Main.scala
[...]
import org.apache.spark.sql.SparkSession
[...]

def createSparkSession(): SparkSession = {
  [...]
  val builder = SparkSession.builder.config(conf)
  [...]
  sparkSession = builder.getOrCreate()
  [...]
  sparkSession
}
```

> 本节就从 `createSparkSession` 函数开始进一步探讨初始化 `SparkSession` 的过程。

> 创建 `builder` 的过程是通过 `SparkSession` 的 `伴生对象` 创建。

```scala
// ${SPARK_HOME}/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala

[...]
@InterfaceStability.Stable
object SparkSession {
  [...]
  @InterfaceStability.Stable
  class Builder extends Logging {
    [...]
    def config(conf: SparkConf): Builder = synchronized {
      conf.getAll.foreach { case (k, v) => options += k -> v }
      this
    }
  }

  [...]
  def builder(): Builder = new Builder
}
```

> `builder.getOrCreate()` 的过程，主要调用 `getOrCreate` 函数。

## 2.1 SparkSession 获取（get）过程

1. 判断当前线程是否存在一个可用的session。
2. 判断全局中是否存在一个可用的session。

> 其中 `options` 是一个可变的（`mutable`）的 `HashMap`，用来存储所有读进来的配置信息。

```scala
// ${SPARK_HOME}/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala

[...]
@InterfaceStability.Stable
object SparkSession {
  [...]
  @InterfaceStability.Stable
  class Builder extends Logging {
    [...]
    def getOrCreate(): SparkSession = synchronized {
      // Get the session from current thread's active session.
      // 判断当前线程是否存在一个可用的session，如果存在则返回该session。
      var session = activeThreadSession.get()
      if ((session ne null) && !session.sparkContext.isStopped) {
        options.foreach { case (k, v) => session.sessionState.conf.setConfString(k, v) }
        if (options.nonEmpty) {
          logWarning("Using an existing SparkSession; some configuration may not take effect.")
        }
        return session
      }

      // Global synchronization so we will only set the default session once.
      // 判断全局中是否存在一个可用的session，如果存在则返回该session。
      SparkSession.synchronized {
        // If the current thread does not have an active session, get it from the global session.
        session = defaultSession.get()
        if ((session ne null) && !session.sparkContext.isStopped) {
          options.foreach { case (k, v) => session.sessionState.conf.setConfString(k, v) }
          if (options.nonEmpty) {
            logWarning("Using an existing SparkSession; some configuration may not take effect.")
          }
          return session
        }
        [...]
      }
      [...]
    }
  }
}
```

## 2.2 SparkSession 创建（create）过程

1. 初始化 `SparkContext`。
2. 如果用户添加自定义 `SparkSession` 的扩展（`SparkSessionExtensions`），则初始化扩展。
3. 将 `SparkContext` 和 `SparkSessionExtensions` 作为入参，初始化 `SparkSession`。
4. 设置一个 `SparkListener` 保证在应用结束时将全局的 `session` 置空。

```scala
// ${SPARK_HOME}/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala

[...]
@InterfaceStability.Stable
object SparkSession {
  [...]
  @InterfaceStability.Stable
  class Builder extends Logging {
    [...]
    def getOrCreate(): SparkSession = synchronized {
      [...]
      SparkSession.synchronized {
        // 初始化 `SparkContext`。
        val sparkContext = userSuppliedContext.getOrElse { [...] }

        [...]
        // 如果用户自定义 `SparkSession` 的扩展（`SPARK_SESSION_EXTENSIONS`），则初始化扩展。
        if (extensionConfOption.isDefined) {
          [...]
          extensionConf(extensions)
        }

        // 将 `SparkContext` 和 `SPARK_SESSION_EXTENSIONS` 作为入参，初始化 `SparkSession`。
        session = new SparkSession(sparkContext, None, None, extensions)
        [...]

        // 设置一个 `SparkListener` 保证在应用结束时将全局的 `session` 置空。
        sparkContext.addSparkListener( [...] )
      }

      return session
    }
  }
}
```

### 2.2.1 初始化 SparkContext

由于初始化 `SparkContext` 的过程过于复杂，在之后章节详细讨论。

### 2.2.2 初始化 SparkSessionExtensions

`SparkSessionExtensions` 主要是注入一些方法去影响 `catalyst`，可以从 [官方文档](https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/sql/SparkSessionExtensions.html) (2.2.1版本) 查询更详细的信息。

```scala
// ${SPARK_HOME}/sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala

[...]
/**
 * :: Experimental ::
 * Holder for injection points to the [[SparkSession]]. We make NO guarantee about the stability
 * regarding binary compatibility and source compatibility of methods here.
 *
 * This current provides the following extension points:
 * - Analyzer Rules.
 * - Check Analysis Rules
 * - Optimizer Rules.
 * - Planning Strategies.
 * - Customized Parser.
 * - (External) Catalog listeners.
 *
 * The extensions can be used by calling withExtension on the [[SparkSession.Builder]], for
 * example:
 * {{{
 *   SparkSession.builder()
 *     .master("...")
 *     .conf("...", true)
 *     .withExtensions { extensions =>
 *       extensions.injectResolutionRule { session =>
 *         ...
 *       }
 *       extensions.injectParser { (session, parser) =>
 *         ...
 *       }
 *     }
 *     .getOrCreate()
 * }}}
 *
 * Note that none of the injected builders should assume that the [[SparkSession]] is fully
 * initialized and should not touch the session's internals (e.g. the SessionState).
 */
@DeveloperApi
@Experimental
@InterfaceStability.Unstable
class SparkSessionExtensions {
  [...]
  private[this] val checkRuleBuilders = mutable.Buffer.empty[CheckRuleBuilder]
  private[this] val optimizerRules = mutable.Buffer.empty[RuleBuilder]
  private[this] val plannerStrategyBuilders = mutable.Buffer.empty[StrategyBuilder]
  private[this] val parserBuilders = mutable.Buffer.empty[ParserBuilder]
  [...]
}

```

### 2.2.3 初始化 SparkSession

现在初始化 `SparkSession` 步骤实际只初始化了一个 `SQLContext`。

```scala
// ${SPARK_HOME}/sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala

[...]
@InterfaceStability.Stable
class SparkSession private(
  @transient val sparkContext: SparkContext,
  @transient private val existingSharedState: Option[SharedState],
  @transient private val parentSessionState: Option[SessionState],
  @transient private[sql] val extensions: SparkSessionExtensions)
extends Serializable with Closeable with Logging { self =>

  [...]

  /**
   * Initial options for session. This options are applied once when sessionState is created.
   */
  @transient
  private[sql] val initialSessionOptions = new scala.collection.mutable.HashMap[String, String]

  [...]

  /**
   * A wrapped version of this session in the form of a [[SQLContext]], for backward compatibility.
   *
   * @since 2.0.0
   */
  @transient
  val sqlContext: SQLContext = new SQLContext(this)

  [...]
}
```

### 2.2.4 注册 SparkListener

在之后章节详细讨论。
